{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor algorithm\n",
    "\n",
    "In this article, we will implement a very basic K-nearest neighbor algorithm and then compare the result with scikit-learn. Please note that this notebook does not go into details how K-nearest neighbors algorithm works because the concept is well explained in many books and articles. There are few resources at the end for your references. But if you are familiar with the theory and trying to code it from the scratch, this is for you!\n",
    "\n",
    "## What does it look like?\n",
    "\n",
    "In the end, we should be able to do the following with the code:\n",
    "- Initialize the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNearestNeighbors(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some basic functionalities that we would expect from a machine learning algorithm. Now, let's dive into the implementation of each function. \n",
    "\n",
    "## Implementation\n",
    "\n",
    "We will have a class that the skeleton looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors(object):\n",
    "    def __init__():\n",
    "        \"\"\" Initialize the classifier \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit():\n",
    "        \"\"\" Train the data \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict():\n",
    "        \"\"\" Make prediction \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During initializing, we will setup some most important hyperparameters for the classifier like:\n",
    "- number of neighbors\n",
    "- distance formula (Manhattan or Euclidean). We will use a generalized form called Minkowski\n",
    "- training data and corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNearestNeighbors(object):\n",
    "    def __init__(self, n_neighbors=1, p=2):\n",
    "        \"\"\" Initialize the classifier \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.p = p # `p` is how we decide the formula for distance calculation\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "    \n",
    "    # The rest ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors is called an **instance-based** or **memory-based** or **lazy learning** algorithm. From [*Wikipedia*](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm):\n",
    "\n",
    "*\"k-NN is a type of **instance-based** learning, or **lazy learning**, where the function is only approximated locally and all computation is deferred until classification.\"*\n",
    "\n",
    "In other words, the training part is not even necessary in this algorithm. This means in our code, we don't need to have a `fit()` method because we can just input the training data, do necessary distance calculations and make prediction (classification) in one single step, that's why the word *lazy*. \n",
    "\n",
    "Eventhough simple, the weakness of this algorithm is that the computation complexity will increase if the number of features increases, and it will be much slower in big dataset because we have to calculate the distance of each new observation to all data points in the training data.\n",
    "\n",
    "One way to overcome this weakness is to use **K-D tree** or **Ball tree**. In production implementation, the training part `fit()` is usually used to construct such intermediate data structures and then used as an input to the prediction part. \n",
    "\n",
    "In this article, we will just simply return the training data from `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors(object):\n",
    "    # other methods\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\" \n",
    "        Train the data. In K-nearest neighbors algorithm, no need to train the data, \n",
    "        we will simply return the input\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    # The rest ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will need to come up with the distance calculation, because it's the core of KNN. We must know in advanced the distance between each new data point to all existing data points in order to make the classification. Again, this is mentioned in textbooks, but here are 3 most common formulas:\n",
    "\n",
    "* Manhattan distance\n",
    "\n",
    "$$d(x, y) = {\\sum_{i=1}^{m} {|x_i - y_i|}}$$\n",
    "\n",
    "\n",
    "* Euclidean distance\n",
    "\n",
    "$$d(x, y) = {\\sqrt {\\sum_{i=1}^{m} {|x_i - y_i|^2}}}$$\n",
    "\n",
    "* Minkowski distance: this is a generalized form for any distance calculation\n",
    "\n",
    "$$d(x, y) = ({\\sum_{i=1}^{m} {|x_i - y_i|^p}})^{\\frac{1}{p}}$$\n",
    "\n",
    "**Note that he bigger it is, the more a large difference in one dimension it will influence the total difference.**\n",
    "\n",
    "In this implementation, we will use the generalized form Minkowski, and set the default to be Euclidean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors(object):\n",
    "    # other methods\n",
    "        \n",
    "    def calculate_distance(self, input_data, new_data):\n",
    "        \"\"\"\n",
    "        A generalized method to calculate distance between new data point \n",
    "        and all existing points in training data based on Minkowski formula.\n",
    "        \n",
    "        self.p = 1 => Manhattan distance\n",
    "        self.p = 2 => Euclidean distance\n",
    "        \n",
    "        Basically we can choose whichever value for `p`, the bigger it is, the more \n",
    "        a large difference in one dimension it will influence the total difference.\n",
    "        \n",
    "        The formula is: distance(X, Y) = (sum of (Xi - Yi)^p ) ^ 1/p (with i=1 to \n",
    "        number of features in input)\n",
    "        \"\"\"\n",
    "        sum_of_power_p = np.sum(np.power(input_data - new_data, self.p), axis=1)\n",
    "        return np.power(sum_of_power_p, 1.0/self.p)   \n",
    "    \n",
    "    # The rest ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction part is now easy. We can apply the `calculate_distance()` method we just coded above to each new data point, sorting the result in ascending order and select the first `n_neighbors`. From the selected result, we calculate the label that appears most frequently, and this will be the label of the new data point.\n",
    "\n",
    "We will have 2 methods:\n",
    "- `predict_single()`: predicts a single data point\n",
    "- `predict()`: broadcast the above method to all other points in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors(object):\n",
    "    # other methods\n",
    "    \n",
    "    def predict_single(self, observation):\n",
    "        \"\"\"\n",
    "        Predict a single observation. The input should be a 1D array that has same \n",
    "        number of features as input except the label. Later this method will be \n",
    "        fetched to the main predict() method for batch processing\n",
    "        \"\"\"\n",
    "        distances = self.calculate_distance(self.X_train, observation)\n",
    "            \n",
    "        label_dist = np.c_[self.y_train, distances]\n",
    "        label_dist = label_dist[label_dist[:, 1].argsort()][:self.n_neighbors]\n",
    "        label, counts = np.unique(label_dist[:, 0], return_counts=True)\n",
    "        label_counts_dict = dict(zip(label, counts))\n",
    "        return max(label_counts_dict.items(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "    def predict(self, observations):\n",
    "        \"\"\"\n",
    "        Predict all data, `observations` is a 2D matrix by applying the predict_single()\n",
    "        method to all rows\n",
    "        \"\"\"\n",
    "        return np.apply_along_axis(self.predict_single, arr=observations, axis=1)\n",
    "    \n",
    "    # the rest ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The whole class\n",
    "\n",
    "This is what the class `KNearestNeighbors` looks like so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors(object):\n",
    "    def __init__(self, n_neighbors=1, p=2):\n",
    "        \"\"\" Initialize the classifier \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.p = p # `p` is how we decide the formula for distance calculation\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\" \n",
    "        Train the data. In K-nearest neighbors algorithm, no need to train the data, \n",
    "        we will simply return the input\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def calculate_distance(self, input_data, new_data):\n",
    "        \"\"\"\n",
    "        A generalized method to calculate distance between new data point \n",
    "        and all existing points in training data based on Minkowski formula.\n",
    "        \n",
    "        self.p = 1 => Manhattan distance\n",
    "        self.p = 2 => Euclidean distance\n",
    "        \n",
    "        Basically we can choose whichever value for `p`, the bigger it is, the more a \n",
    "        large difference in one dimension it will influence the total difference.\n",
    "        \n",
    "        The formula is: distance(X, Y) = (sum of (Xi - Yi)^p ) ^ 1/p (with i=1 to \n",
    "        number of features in input)\n",
    "        \"\"\"\n",
    "        sum_of_power_p = np.sum(np.power(input_data - new_data, self.p), axis=1)\n",
    "        return np.power(sum_of_power_p, 1.0/self.p)\n",
    "    \n",
    "    def predict_single(self, observation):\n",
    "        \"\"\"\n",
    "        Predict a single observation. The input should be a 1D array that has \n",
    "        same number of features as input except the label. Later this method \n",
    "        will be fetched to the main predict() method for batch processing\n",
    "        \"\"\"\n",
    "        distances = self.calculate_distance(self.X_train, observation)\n",
    "            \n",
    "        label_dist = np.c_[self.y_train, distances]\n",
    "        label_dist = label_dist[label_dist[:, 1].argsort()][:self.n_neighbors]\n",
    "        label, counts = np.unique(label_dist[:, 0], return_counts=True)\n",
    "        label_counts_dict = dict(zip(label, counts))\n",
    "        return max(label_counts_dict.items(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "    def predict(self, observations):\n",
    "        \"\"\"\n",
    "        Predict all data, `observations` is a 2D matrix by applying the predict_single()\n",
    "        method to all rows\n",
    "        \"\"\"\n",
    "        return np.apply_along_axis(self.predict_single, arr=observations, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision\n",
    "\n",
    "We will check the accuracy of our algorithm by applying it on the iris dataset, and then crosscheck the result with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and test set, 30% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do our implementation perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRIS DATASET\n",
      "Accuracy score for our implementation of KNN with k=1: 0.96%\n",
      "Accuracy score for our implementation of KNN with k=2: 0.98%\n",
      "Accuracy score for our implementation of KNN with k=3: 0.98%\n",
      "Accuracy score for our implementation of KNN with k=4: 0.98%\n",
      "Accuracy score for our implementation of KNN with k=5: 1.00%\n",
      "Accuracy score for our implementation of KNN with k=6: 0.98%\n",
      "Accuracy score for our implementation of KNN with k=7: 1.00%\n",
      "Accuracy score for our implementation of KNN with k=8: 0.96%\n",
      "Accuracy score for our implementation of KNN with k=9: 0.98%\n",
      "Accuracy score for our implementation of KNN with k=10: 0.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"IRIS DATASET\")\n",
    "for k in range(1, 11):\n",
    "    knn_scratch = KNearestNeighbors(n_neighbors=k)\n",
    "    knn_scratch.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_scratch = knn_scratch.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy score for our implementation of KNN with k={}: {:.2f}%\"\n",
    "          .format(k, accuracy_score(y_pred_scratch, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to scikit-learn perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRIS DATASET\n",
      "Accuracy score for scikit-learn implementation of KNN with k=1: 0.98%\n",
      "Accuracy score for scikit-learn implementation of KNN with k=2: 0.98%\n",
      "Accuracy score for scikit-learn implementation of KNN with k=3: 0.98%\n",
      "Accuracy score for scikit-learn implementation of KNN with k=4: 0.98%\n",
      "Accuracy score for scikit-learn implementation of KNN with k=5: 0.98%\n",
      "Accuracy score for scikit-learn implementation of KNN with k=6: 0.98%\n",
      "Accuracy score for scikit-learn implementation of KNN with k=7: 0.98%\n",
      "Accuracy score for scikit-learn implementation of KNN with k=8: 0.98%\n",
      "Accuracy score for scikit-learn implementation of KNN with k=9: 0.98%\n",
      "Accuracy score for scikit-learn implementation of KNN with k=10: 0.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"IRIS DATASET\")\n",
    "for k in range(1, 11):\n",
    "    knn_sk = KNeighborsClassifier(n_neighbors=9)\n",
    "    knn_sk.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_sk = knn_sk.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy score for scikit-learn implementation of KNN with k={}: {:.2f}%\"\n",
    "          .format(k, accuracy_score(y_pred_sk, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation of KNN performs quite good compared to the scikit-learn library even though we didn't do anything fancy.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "As mentioned above, the `fit()` method does nothing but returning the training data and their labels. As a next step we can try to improve this step by implementing **K-D tree** or **Ball tree** that can speed up the classification done in prediction part.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- Vietnamese\n",
    "  - [BÃ i 6: K-nearest neighbors](https://machinelearningcoban.com/2017/01/08/knn/) (machinelearningcoban.com)\n",
    "- English\n",
    "  - [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "  - [Type of distance functions](https://www.ijsr.net/archive/v4i7/SUB156942.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
